# 对比学习（Contrastive Learning）

- **无监督**表示学习方法(利用无标签数据进行表示学习的思想)

## 一. 基础概念

### 核心

- 用处：学习同类之间的共同特征，区分非同类实例之间的不同之处。在抽象语义级别的特征空间上学会区分数据

- 目标：是学习一个编码器，此编码器对同类数据进行相似的编码，并使得不同类的数据的编码尽可能不同。

- 核心：学习$f$这个映射函数，把实例$x$编码成$f(x)$，使得$f$满足：$$ s(f(x),f(x^+)) >> s(f(x),f(x^-))$$

  其中$x^+$是和$x$相似的实例，$x^-$是和$x$不相似的实例，$s(.,.)$是度量实例之间相似度的函数。--> **损失函数**

### 组成

- **正负样例**：正样例指的是与给定样例语义相近的样例，而负样例指的是与给定样例语义不相近的样例。

  - 有监督的时候，正负样例很容易构造，同一标签下的数据互为正样例，不同标签下数据互为负样例
  - **存在一个问题：**无监督的时候如何得到正负样本
    - 目前的主流做法是对所有样例**数据增强**，产生一些新的样例，同一个样例扰动产生的所有样例之间互为正样例，不同样例扰动产生的样例彼此之间互为负样例。
    - **图像：**空间/几何扰动和外观/色彩扰动。空间/几何扰动的方式包括但不限于图片翻转（flip）、图片旋转（rotation）、图片挖剪（cutout）、图片剪切并放大（crop and resize）。外观扰动包括但不限于色彩失真、加高斯噪声等
    - **文本：**词级别（token-level）和表示级别（embedding-level）。词级别的扰动大致有句子剪裁（crop）、删除词/词块（span）、换序、同义词替换等。表示级别的扰动包括加高斯噪声、dropout等。（**但是不同于图像领域，对自然语言的扰动很容易改变语义，这就会引入错误正例（False Positive）从而对模型训练产生不好的影响。**）

- **对比损失**

  - **原始的损失函数：**

    - $L((x_1,x_2),y)=yS_p(f(x),f(x^+)) + (1-y)S_n(f(x),f(x^-))$，其中$(x_1,x_2)$为一个样例对(为正或负)，$y\in{0,1}$，$S_p$为递增函数，$S_n$为递减函数

  - **三元组损失函数（Triplet Loss）:**

    - $L(x,x^+,x^-)=max(0,S(x,x^+)^2-S(x,x^-)^2+m)$，x为锚点(Anchor)，m是用来控制正例负例距离间的偏离量，使模型不需要考虑优化过于简单的负例。

    - 虽然triplet loss已经满足了对比学习的要求，但是他把一个样例限定在了一个三元组中，一个正例只与一个负例对比。实际操作中，对一个样例，我们能得到的负样例个数远远多于正样例，为了利用这些资源，近年来对比学习多用**InfoNCE loss**

  - **NCE**（Noise Contrastive Estimate）to train --> sigmoid cross entropy loss to test（源于softmax归一化计算代价过高的问题[link](https://ruder.io/word-embeddings-softmax)）

    - 噪声对比估计：引入NCE，解决多分类问题中softmax分母归一化中分母难以求值的问题 --> 把多分类通过引入一个噪声分布变化转为为一个二元分类器，用于判别给定样例式来源于原始分布还是噪声部分，进而更新原来多元分类器的参数。
    - 这里将**负样本**同比与**噪声部分**

  - **InfoNCE loss（Noise Contrastive Estimate）：**

    - InfoNCE继承了NCE的基本思想，从一个新的分布引入负样例，构造了一个新的多元分类问题，并且证明了减小这个损失函数相当于增大互信息(mutual information)的下界
    - $L(x,x^+,x^-)=-log\frac {exp(x·x^+/\alpha)}{\sum_v\in x^+,x^-exp(x·v / \alpha)}$
    - infoNCE最后的形式就是多元分类任务常见的交叉熵（cross entropy）**softmax 损失**
    - 在拉近原样例与正样例距离的同时，拉远其与负样例间的距离
    - 根据infoNCE loss的特性，**一般希望能提供尽量多的负样例进行优化**，这样不但可以增加负例的多样性，同时可以提供更多的hard negatives，提升最终表现。

- **模型结构**

### 对齐性(alignment)和均匀性(uniformity)

- 正则化后引入的**对齐性(alignment)和均匀性(uniformity)**
  - 对比学习的表示一般会进行正则化，因而会集中在一个超球面上。
  - **对齐性：**一个相似的样例的表示尽量接近
  - **均匀性：**不相似样例的表示应该均匀的分布在超球面上
- 空间的好坏：通过对齐性和均匀性

## 二. 图像对比学习

- 样例：不同角度的照片，相同的接近，不同的远离，使得学到的表示可**忽略角度变换带来的细节变动**，进而学到更高维度、更本质的语义信息。

### Memory Bank

- **缘由：**使用infoNCE loss的时候，一般希望能提供尽量多的负样例及逆行优化，但运算能力有限
- **思想：**在开始训练之前，先将所有图片的表示计算好储存起来

### MoCo v1

### SimCLR

### BYOL

### SimSiam

## 三. 文本对比学习

- (样例)与(负样例)对比，尽可能不相似
- (样例)与(正样例)对比，尽可能相似
- 相似--> 语义空间更加接近

### ConSERT

利用对比学习思想进行句表示学习，在语义文本相似度匹配（STS）等任务上超过了SOTA

### SimCSE

## 图对比学习

### MoCL:Contrastive Learning on Molecular Graphs with Multi-level Domain Knowledge

> 【KDD2021】多层次领域知识在分子图上的对比学习

- GNN是数据需求性，但在现实世界中获取有标签的数据是非常昂贵的，但以一种无监督的方式对GNN进行预处理是现有的探索
- 图对比学习通过最大化成对图增强之间的互信息，已被证明对各种下游任务是有效的。
- 现有的图对比学习框架局限性：
  - 增强是为一般图设计的，因此对于某些领域可能不能适合或不够强大
  - 对比方案只学习对局部扰动不变的表示，一i那次不考虑数据集的全局结构
- 本文主要针对生物医学领域存在分子图的图对比学习，提出了新的框架MoCL：
  - 利用领域知识在局部或全局水平上帮助表示学习
    - 局部层次的领域知识扩展过程，这样在不改变图语义的情况下引入变体
    - 全局层次的知识对整个数据集图之间的相似性信息进行编码，并帮助学习具有更丰富语义的表示。
  - 整个模型通过双对比目标学习。



