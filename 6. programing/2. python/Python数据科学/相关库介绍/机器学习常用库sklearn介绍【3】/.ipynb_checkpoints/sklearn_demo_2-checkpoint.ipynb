{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#数据处理\" data-toc-modified-id=\"数据处理-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>数据处理</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standardization,-or-mean-removal-and-variance-scaling\" data-toc-modified-id=\"Standardization,-or-mean-removal-and-variance-scaling-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Standardization, or mean removal and variance scaling</a></span></li><li><span><a href=\"#Normalization\" data-toc-modified-id=\"Normalization-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Normalization</a></span></li><li><span><a href=\"#Binarization（离散化）\" data-toc-modified-id=\"Binarization（离散化）-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Binarization（离散化）</a></span></li><li><span><a href=\"#Encoding-categorical-features\" data-toc-modified-id=\"Encoding-categorical-features-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Encoding categorical features</a></span></li><li><span><a href=\"#Imputation-of-missing-values\" data-toc-modified-id=\"Imputation-of-missing-values-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Imputation of missing values</a></span></li></ul></li><li><span><a href=\"#特征选择\" data-toc-modified-id=\"特征选择-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>特征选择</a></span><ul class=\"toc-item\"><li><span><a href=\"#SelectFromModel\" data-toc-modified-id=\"SelectFromModel-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>SelectFromModel</a></span></li><li><span><a href=\"#※各特征独立考量\" data-toc-modified-id=\"※各特征独立考量-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>※各特征独立考量</a></span><ul class=\"toc-item\"><li><span><a href=\"#Removing-features-with-low-variance\" data-toc-modified-id=\"Removing-features-with-low-variance-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Removing features with low variance</a></span></li><li><span><a href=\"#Univariate-feature-selection\" data-toc-modified-id=\"Univariate-feature-selection-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Univariate feature selection</a></span></li></ul></li></ul></li><li><span><a href=\"#降维（考虑了所有特征间的整体贡献）\" data-toc-modified-id=\"降维（考虑了所有特征间的整体贡献）-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>降维（考虑了所有特征间的整体贡献）</a></span><ul class=\"toc-item\"><li><span><a href=\"#主成分分析PCA\" data-toc-modified-id=\"主成分分析PCA-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>主成分分析PCA</a></span></li><li><span><a href=\"#Truncated-SVD（截断的奇异矩阵分解）\" data-toc-modified-id=\"Truncated-SVD（截断的奇异矩阵分解）-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Truncated SVD（截断的奇异矩阵分解）</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn模块基础学习【2】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "快速入门参考学习文档：  \n",
    "  \n",
    ">  https://sklearn.apachecn.org/docs/0.21.3/      \n",
    ">  https://sklearn.apachecn.org/docs/0.21.3/50.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集：[ML DATASETS](http://archive.ics.uci.edu/ml/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization, or mean removal and variance scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, **_then scale it by dividing non-constant features by their standard deviation_**.\n",
    "\n",
    "For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n",
    "\n",
    "[Should I normalize/standardize/rescale the data](http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html,\"Should I normalize/standardize/rescale the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**StandardScaler**](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)\n",
    "\n",
    "The preprocessing module further provides a utility class StandardScaler that implements the Transformer API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set.\n",
    "\n",
    "\n",
    "[**MinMaxScaler**](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)\n",
    "\n",
    "Scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>oct</td>\n",
       "      <td>tue</td>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>oct</td>\n",
       "      <td>sat</td>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>mar</td>\n",
       "      <td>sun</td>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X  Y month  day  FFMC   DMC     DC  ISI  temp  RH  wind  rain  area\n",
       "0  7  5   mar  fri  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0   0.0\n",
       "1  7  4   oct  tue  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0   0.0\n",
       "2  7  4   oct  sat  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0   0.0\n",
       "3  8  6   mar  fri  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2   0.0\n",
       "4  8  6   mar  sun  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0   0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.read_csv(\"forestfires.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FFMC   DMC     DC  ISI  temp  RH  wind  rain\n",
       "0  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0\n",
       "1  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0\n",
       "2  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0\n",
       "3  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2\n",
       "4  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.loc[:,\"FFMC\":\"rain\"]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 70% train, 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df1.astype(float), df['area'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "ss.fit(X_train.astype(float))\n",
    "\n",
    "X_train_ss = ss.transform(X_train)\n",
    "X_test_ss = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19833798,  0.10387793,  0.15785763, ...,  2.44483192,\n",
       "         1.94361423, -0.08050154],\n",
       "       [-0.41459806,  2.50988941,  1.02064002, ..., -1.03279191,\n",
       "        -0.2445808 , -0.08050154],\n",
       "       [ 0.31184465,  1.6142209 ,  0.54338085, ...,  0.61450358,\n",
       "        -1.01044906, -0.08050154],\n",
       "       ...,\n",
       "       [ 1.08369003, -0.0956917 , -0.07322406, ..., -0.78874814,\n",
       "         0.24776308, -0.08050154],\n",
       "       [ 0.03942863, -1.38890292, -1.94161823, ...,  0.24843792,\n",
       "         0.74010696, -0.08050154],\n",
       "       [ 0.92478069,  0.03841909,  0.44467761, ..., -1.33784664,\n",
       "         0.24776308, -0.08050154]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156, 8) (156, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape, X_test_ss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06141768,  0.26343129,  0.20168851,  0.16762858,  0.14628027,\n",
       "        0.0728359 , -0.05346441, -0.06223793])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_ss.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.69632577, 1.04685974, 0.84088763, 1.48284975, 0.99770054,\n",
       "       0.98003792, 0.92811413, 0.1872176 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_ss.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "mms.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91266376, 0.34735744, 0.87750767, ..., 0.20238095, 0.15294118,\n",
       "        0.        ],\n",
       "       [0.88646288, 0.44193324, 0.80800094, ..., 0.52380952, 0.63529412,\n",
       "        0.        ],\n",
       "       [0.90174672, 0.11613352, 0.08602785, ..., 0.14285714, 0.47058824,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.87991266, 0.14464534, 0.09971678, ..., 0.53571429, 0.36470588,\n",
       "        0.        ],\n",
       "       [0.87772926, 0.14360223, 0.80127449, ..., 0.11904762, 0.25882353,\n",
       "        0.        ],\n",
       "       [0.94104803, 0.47635605, 0.69188105, ..., 0.44047619, 0.57647059,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mms.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Normalization\n",
    "\n",
    "Normalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples.\n",
    "\n",
    "This assumption is the base of the Vector Space Model often used in text classification and clustering contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizer类也拥有fit、transform等转换器API拥有的常见方法，但实际上fit和transform对其是没有实际意义的，因为归一化操作是对每个样本单独进行变换，不存在针对所有样本上的统计学习过程。这里的设计，仅仅是为了供sklearn中的pipeline等API调用时，传入该对象时，各API的方法能够保持一致性，方便使用pipeline。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "norm = Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12051853, 0.13372067, 0.98231751, ..., 0.04182856, 0.00287571,\n",
       "        0.        ],\n",
       "       [0.12761217, 0.18160194, 0.97125486, ..., 0.08273756, 0.00883469,\n",
       "        0.        ],\n",
       "       [0.69673891, 0.2720093 , 0.61392044, ..., 0.20514668, 0.03723032,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.60495241, 0.29347195, 0.6162911 , ..., 0.40018902, 0.02667927,\n",
       "        0.        ],\n",
       "       [0.13036744, 0.06288143, 0.98840393, ..., 0.03597336, 0.0044607 ,\n",
       "        0.        ],\n",
       "       [0.15074612, 0.22474876, 0.9580037 , ..., 0.08383742, 0.0093511 ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.fit(X_train)\n",
    "X_train_norm = norm.transform(X_train)\n",
    "X_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.square(X_train_norm[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarization（离散化）\n",
    "\n",
    "Feature binarization is the process of thresholding numerical features to get boolean values. This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution. For instance, this is the case for the sklearn.neural_network.BernoulliRBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "547.9400386847191"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DC'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "bi = Binarizer(548)\n",
    "DC_bi = bi.fit_transform(df[['DC']])\n",
    "DC_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DC_bi'] = DC_bi[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>DC_bi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>oct</td>\n",
       "      <td>tue</td>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>oct</td>\n",
       "      <td>sat</td>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>mar</td>\n",
       "      <td>sun</td>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X  Y month  day  FFMC   DMC     DC  ISI  temp  RH  wind  rain  area  DC_bi\n",
       "0  7  5   mar  fri  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0   0.0    0.0\n",
       "1  7  4   oct  tue  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0   0.0    1.0\n",
       "2  7  4   oct  sat  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0   0.0    1.0\n",
       "3  8  6   mar  fri  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2   0.0    0.0\n",
       "4  8  6   mar  sun  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0   0.0    0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return indices of half-open bins to which each value of x belongs.\n",
    "```python\n",
    "pandas.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False)\n",
    "```  \n",
    "pandas.cut是按分位数划分的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       (7.047, 178.44]\n",
       "1      (519.52, 690.06]\n",
       "2      (519.52, 690.06]\n",
       "3       (7.047, 178.44]\n",
       "4       (7.047, 178.44]\n",
       "5      (348.98, 519.52]\n",
       "6      (348.98, 519.52]\n",
       "7      (519.52, 690.06]\n",
       "8       (690.06, 860.6]\n",
       "9       (690.06, 860.6]\n",
       "10      (690.06, 860.6]\n",
       "11      (690.06, 860.6]\n",
       "12     (519.52, 690.06]\n",
       "13     (519.52, 690.06]\n",
       "14      (690.06, 860.6]\n",
       "15      (690.06, 860.6]\n",
       "16      (7.047, 178.44]\n",
       "17     (519.52, 690.06]\n",
       "18      (7.047, 178.44]\n",
       "19      (7.047, 178.44]\n",
       "20      (690.06, 860.6]\n",
       "21      (690.06, 860.6]\n",
       "22     (178.44, 348.98]\n",
       "23     (519.52, 690.06]\n",
       "24     (519.52, 690.06]\n",
       "25     (519.52, 690.06]\n",
       "26     (519.52, 690.06]\n",
       "27     (519.52, 690.06]\n",
       "28      (690.06, 860.6]\n",
       "29      (690.06, 860.6]\n",
       "             ...       \n",
       "487    (519.52, 690.06]\n",
       "488    (519.52, 690.06]\n",
       "489    (519.52, 690.06]\n",
       "490    (519.52, 690.06]\n",
       "491    (519.52, 690.06]\n",
       "492    (519.52, 690.06]\n",
       "493    (519.52, 690.06]\n",
       "494    (519.52, 690.06]\n",
       "495    (519.52, 690.06]\n",
       "496    (519.52, 690.06]\n",
       "497    (519.52, 690.06]\n",
       "498    (519.52, 690.06]\n",
       "499    (519.52, 690.06]\n",
       "500    (519.52, 690.06]\n",
       "501    (519.52, 690.06]\n",
       "502    (519.52, 690.06]\n",
       "503    (519.52, 690.06]\n",
       "504    (519.52, 690.06]\n",
       "505     (690.06, 860.6]\n",
       "506     (690.06, 860.6]\n",
       "507     (690.06, 860.6]\n",
       "508     (690.06, 860.6]\n",
       "509     (690.06, 860.6]\n",
       "510     (690.06, 860.6]\n",
       "511    (519.52, 690.06]\n",
       "512    (519.52, 690.06]\n",
       "513    (519.52, 690.06]\n",
       "514    (519.52, 690.06]\n",
       "515    (519.52, 690.06]\n",
       "516     (7.047, 178.44]\n",
       "Name: DC, Length: 517, dtype: category\n",
       "Categories (5, interval[float64]): [(7.047, 178.44] < (178.44, 348.98] < (348.98, 519.52] < (519.52, 690.06] < (690.06, 860.6]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.cut(df['DC'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical features\n",
    "\n",
    "We could encode categorical features as integers, but such integer representation can not be used directly with scikit-learn estimators, as these expect continuous input, and would interpret the categories as being ordered, which is often not desired.\n",
    "\n",
    "One possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K or one-hot encoding, which is implemented in [OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder). This estimator transforms each categorical feature with m possible values into m binary features, with only one active."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OneHotEncoder**(一般不用)\n",
    "```python\n",
    "class sklearn.preprocessing.OneHotEncoder(n_values='auto', categorical_features='all', dtype=<type 'numpy.float64'>, \n",
    "                                          sparse=True, handle_unknown='error')```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert categorical variable into dummy/indicator variables\n",
    "```python\n",
    "pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>...</th>\n",
       "      <th>month_nov</th>\n",
       "      <th>month_oct</th>\n",
       "      <th>month_sep</th>\n",
       "      <th>day_fri</th>\n",
       "      <th>day_mon</th>\n",
       "      <th>day_sat</th>\n",
       "      <th>day_sun</th>\n",
       "      <th>day_thu</th>\n",
       "      <th>day_tue</th>\n",
       "      <th>day_wed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   X  Y  FFMC   DMC     DC  ISI  temp  RH  wind  rain  ...  month_nov  \\\n",
       "0  7  5  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0  ...          0   \n",
       "1  7  4  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0  ...          0   \n",
       "2  7  4  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0  ...          0   \n",
       "3  8  6  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2  ...          0   \n",
       "4  8  6  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0  ...          0   \n",
       "\n",
       "   month_oct  month_sep  day_fri  day_mon  day_sat  day_sun  day_thu  day_tue  \\\n",
       "0          0          0        1        0        0        0        0        0   \n",
       "1          1          0        0        0        0        0        0        1   \n",
       "2          1          0        0        0        1        0        0        0   \n",
       "3          0          0        1        0        0        0        0        0   \n",
       "4          0          0        0        0        0        1        0        0   \n",
       "\n",
       "   day_wed  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelData = pd.get_dummies(data=df, columns=['month','day'])\n",
    "modelData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Imputation of missing values\n",
    "\n",
    "For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). **_A better strategy is to impute the missing values, i.e., to infer them from the known part of the data._**\n",
    "\n",
    "\n",
    "The [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer) class provides basic strategies for imputing missing values, either using the mean, the median or the most frequent value of the row or column in which the missing values are located. This class also allows for different missing values encodings.\n",
    "\n",
    "\n",
    "\n",
    "**The imputation strategy:**\n",
    "1. If “mean”, then replace missing values using the mean along the axis.\n",
    "2. If “median”, then replace missing values using the median along the axis.\n",
    "3. If “most_frequent”, then replace missing using the most frequent value along the axis.\n",
    "4. If “constant”, then replace missing values with fill_value. Can be used with strings or numeric data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class sklearn.impute.SimpleImputer(missing_values=nan, strategy='mean', fill_value=None, verbose=0, copy=True, add_indicator=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'DC_na'] = np.nan\n",
    "df.loc[df['DC']>=600, 'DC_na'] = df['DC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "im = SimpleImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>DC_bi</th>\n",
       "      <th>DC_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>oct</td>\n",
       "      <td>tue</td>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>669.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>oct</td>\n",
       "      <td>sat</td>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>686.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>mar</td>\n",
       "      <td>sun</td>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X  Y month  day  FFMC   DMC     DC  ISI  temp  RH  wind  rain  area  DC_bi  \\\n",
       "0  7  5   mar  fri  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0   0.0    0.0   \n",
       "1  7  4   oct  tue  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0   0.0    1.0   \n",
       "2  7  4   oct  sat  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0   0.0    1.0   \n",
       "3  8  6   mar  fri  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2   0.0    0.0   \n",
       "4  8  6   mar  sun  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0   0.0    0.0   \n",
       "\n",
       "   DC_na  \n",
       "0    NaN  \n",
       "1  669.1  \n",
       "2  686.9  \n",
       "3    NaN  \n",
       "4    NaN  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[703.07754491],\n",
       "       [669.1       ],\n",
       "       [686.9       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [608.2       ],\n",
       "       [692.6       ],\n",
       "       [698.6       ],\n",
       "       [698.6       ],\n",
       "       [713.        ],\n",
       "       [665.3       ],\n",
       "       [686.5       ],\n",
       "       [699.6       ],\n",
       "       [713.9       ],\n",
       "       [703.07754491],\n",
       "       [664.2       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [692.6       ],\n",
       "       [724.3       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [601.4       ],\n",
       "       [668.        ],\n",
       "       [686.5       ],\n",
       "       [721.4       ],\n",
       "       [728.6       ],\n",
       "       [692.3       ],\n",
       "       [709.9       ],\n",
       "       [706.8       ],\n",
       "       [718.3       ],\n",
       "       [724.3       ],\n",
       "       [730.2       ],\n",
       "       [669.1       ],\n",
       "       [682.6       ],\n",
       "       [686.9       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [624.2       ],\n",
       "       [647.1       ],\n",
       "       [698.6       ],\n",
       "       [735.7       ],\n",
       "       [692.3       ],\n",
       "       [686.5       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [706.4       ],\n",
       "       [631.2       ],\n",
       "       [654.1       ],\n",
       "       [654.1       ],\n",
       "       [661.3       ],\n",
       "       [706.4       ],\n",
       "       [730.2       ],\n",
       "       [691.8       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [631.2       ],\n",
       "       [638.8       ],\n",
       "       [661.3       ],\n",
       "       [668.        ],\n",
       "       [668.        ],\n",
       "       [668.        ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [692.3       ],\n",
       "       [703.07754491],\n",
       "       [614.5       ],\n",
       "       [713.9       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [601.4       ],\n",
       "       [631.2       ],\n",
       "       [647.1       ],\n",
       "       [654.1       ],\n",
       "       [661.3       ],\n",
       "       [706.4       ],\n",
       "       [706.4       ],\n",
       "       [706.4       ],\n",
       "       [728.6       ],\n",
       "       [703.07754491],\n",
       "       [624.2       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [601.4       ],\n",
       "       [638.8       ],\n",
       "       [704.4       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [601.4       ],\n",
       "       [601.4       ],\n",
       "       [601.4       ],\n",
       "       [614.5       ],\n",
       "       [647.1       ],\n",
       "       [674.4       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [631.2       ],\n",
       "       [698.6       ],\n",
       "       [709.9       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [704.4       ],\n",
       "       [724.3       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [608.2       ],\n",
       "       [608.2       ],\n",
       "       [680.7       ],\n",
       "       [671.9       ],\n",
       "       [692.3       ],\n",
       "       [691.8       ],\n",
       "       [703.07754491],\n",
       "       [728.6       ],\n",
       "       [673.8       ],\n",
       "       [691.8       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [685.2       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [680.7       ],\n",
       "       [686.5       ],\n",
       "       [703.07754491],\n",
       "       [692.6       ],\n",
       "       [686.5       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [671.9       ],\n",
       "       [647.1       ],\n",
       "       [685.2       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [692.3       ],\n",
       "       [721.4       ],\n",
       "       [647.1       ],\n",
       "       [721.4       ],\n",
       "       [654.1       ],\n",
       "       [654.1       ],\n",
       "       [668.        ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [674.4       ],\n",
       "       [704.4       ],\n",
       "       [703.07754491],\n",
       "       [654.1       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [699.6       ],\n",
       "       [609.6       ],\n",
       "       [601.4       ],\n",
       "       [686.5       ],\n",
       "       [624.2       ],\n",
       "       [624.2       ],\n",
       "       [703.07754491],\n",
       "       [631.2       ],\n",
       "       [735.7       ],\n",
       "       [614.5       ],\n",
       "       [680.7       ],\n",
       "       [664.2       ],\n",
       "       [703.07754491],\n",
       "       [696.1       ],\n",
       "       [703.07754491],\n",
       "       [692.6       ],\n",
       "       [703.07754491],\n",
       "       [686.5       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [647.1       ],\n",
       "       [699.6       ],\n",
       "       [647.1       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [706.4       ],\n",
       "       [692.6       ],\n",
       "       [665.3       ],\n",
       "       [692.6       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [673.8       ],\n",
       "       [703.07754491],\n",
       "       [706.4       ],\n",
       "       [703.07754491],\n",
       "       [692.6       ],\n",
       "       [668.        ],\n",
       "       [685.2       ],\n",
       "       [686.9       ],\n",
       "       [703.07754491],\n",
       "       [692.3       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [680.7       ],\n",
       "       [709.9       ],\n",
       "       [699.6       ],\n",
       "       [703.07754491],\n",
       "       [631.2       ],\n",
       "       [713.9       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [735.7       ],\n",
       "       [728.6       ],\n",
       "       [696.1       ],\n",
       "       [703.07754491],\n",
       "       [728.6       ],\n",
       "       [703.07754491],\n",
       "       [699.6       ],\n",
       "       [728.6       ],\n",
       "       [692.6       ],\n",
       "       [671.9       ],\n",
       "       [674.4       ],\n",
       "       [601.4       ],\n",
       "       [674.4       ],\n",
       "       [692.6       ],\n",
       "       [674.4       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [700.7       ],\n",
       "       [700.7       ],\n",
       "       [700.7       ],\n",
       "       [700.7       ],\n",
       "       [703.07754491],\n",
       "       [666.7       ],\n",
       "       [666.7       ],\n",
       "       [666.7       ],\n",
       "       [666.7       ],\n",
       "       [666.7       ],\n",
       "       [703.07754491],\n",
       "       [621.7       ],\n",
       "       [694.8       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [692.3       ],\n",
       "       [692.3       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [629.1       ],\n",
       "       [684.4       ],\n",
       "       [703.07754491],\n",
       "       [607.1       ],\n",
       "       [658.2       ],\n",
       "       [658.2       ],\n",
       "       [658.2       ],\n",
       "       [658.2       ],\n",
       "       [658.2       ],\n",
       "       [658.2       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [714.3       ],\n",
       "       [714.3       ],\n",
       "       [714.3       ],\n",
       "       [758.1       ],\n",
       "       [758.1       ],\n",
       "       [758.1       ],\n",
       "       [758.1       ],\n",
       "       [706.6       ],\n",
       "       [777.1       ],\n",
       "       [777.1       ],\n",
       "       [817.5       ],\n",
       "       [739.4       ],\n",
       "       [739.4       ],\n",
       "       [783.5       ],\n",
       "       [783.5       ],\n",
       "       [783.5       ],\n",
       "       [783.5       ],\n",
       "       [783.5       ],\n",
       "       [822.8       ],\n",
       "       [726.9       ],\n",
       "       [751.5       ],\n",
       "       [751.5       ],\n",
       "       [751.5       ],\n",
       "       [751.5       ],\n",
       "       [751.5       ],\n",
       "       [751.5       ],\n",
       "       [795.3       ],\n",
       "       [795.3       ],\n",
       "       [721.1       ],\n",
       "       [764.        ],\n",
       "       [764.        ],\n",
       "       [764.        ],\n",
       "       [764.        ],\n",
       "       [764.        ],\n",
       "       [764.        ],\n",
       "       [764.        ],\n",
       "       [770.3       ],\n",
       "       [807.1       ],\n",
       "       [807.1       ],\n",
       "       [807.1       ],\n",
       "       [807.1       ],\n",
       "       [807.1       ],\n",
       "       [745.3       ],\n",
       "       [745.3       ],\n",
       "       [745.3       ],\n",
       "       [745.3       ],\n",
       "       [745.3       ],\n",
       "       [745.3       ],\n",
       "       [745.3       ],\n",
       "       [745.3       ],\n",
       "       [745.3       ],\n",
       "       [745.3       ],\n",
       "       [789.7       ],\n",
       "       [789.7       ],\n",
       "       [789.7       ],\n",
       "       [789.7       ],\n",
       "       [789.7       ],\n",
       "       [732.3       ],\n",
       "       [770.3       ],\n",
       "       [770.3       ],\n",
       "       [770.3       ],\n",
       "       [812.1       ],\n",
       "       [812.1       ],\n",
       "       [744.4       ],\n",
       "       [825.1       ],\n",
       "       [825.1       ],\n",
       "       [703.07754491],\n",
       "       [664.5       ],\n",
       "       [698.6       ],\n",
       "       [855.3       ],\n",
       "       [744.4       ],\n",
       "       [672.6       ],\n",
       "       [715.1       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [643.        ],\n",
       "       [690.        ],\n",
       "       [753.8       ],\n",
       "       [819.1       ],\n",
       "       [613.        ],\n",
       "       [750.5       ],\n",
       "       [703.07754491],\n",
       "       [706.7       ],\n",
       "       [706.7       ],\n",
       "       [703.07754491],\n",
       "       [738.1       ],\n",
       "       [825.1       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [750.5       ],\n",
       "       [613.        ],\n",
       "       [715.1       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [731.7       ],\n",
       "       [706.7       ],\n",
       "       [643.        ],\n",
       "       [725.1       ],\n",
       "       [680.9       ],\n",
       "       [860.6       ],\n",
       "       [703.07754491],\n",
       "       [855.3       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [715.1       ],\n",
       "       [723.1       ],\n",
       "       [698.6       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [664.5       ],\n",
       "       [613.        ],\n",
       "       [635.9       ],\n",
       "       [690.        ],\n",
       "       [795.9       ],\n",
       "       [744.4       ],\n",
       "       [715.1       ],\n",
       "       [753.8       ],\n",
       "       [753.8       ],\n",
       "       [672.6       ],\n",
       "       [698.6       ],\n",
       "       [613.        ],\n",
       "       [849.3       ],\n",
       "       [605.3       ],\n",
       "       [698.6       ],\n",
       "       [723.1       ],\n",
       "       [811.2       ],\n",
       "       [703.07754491],\n",
       "       [672.6       ],\n",
       "       [768.4       ],\n",
       "       [715.1       ],\n",
       "       [738.1       ],\n",
       "       [855.3       ],\n",
       "       [672.6       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [855.3       ],\n",
       "       [703.07754491],\n",
       "       [664.5       ],\n",
       "       [703.07754491],\n",
       "       [844.        ],\n",
       "       [613.        ],\n",
       "       [690.        ],\n",
       "       [649.9       ],\n",
       "       [730.6       ],\n",
       "       [803.3       ],\n",
       "       [753.8       ],\n",
       "       [703.07754491],\n",
       "       [753.8       ],\n",
       "       [635.9       ],\n",
       "       [715.1       ],\n",
       "       [819.1       ],\n",
       "       [715.1       ],\n",
       "       [715.1       ],\n",
       "       [825.1       ],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [703.07754491],\n",
       "       [605.8       ],\n",
       "       [605.8       ],\n",
       "       [605.8       ],\n",
       "       [605.8       ],\n",
       "       [605.8       ],\n",
       "       [624.1       ],\n",
       "       [633.6       ],\n",
       "       [633.6       ],\n",
       "       [643.        ],\n",
       "       [661.8       ],\n",
       "       [661.8       ],\n",
       "       [671.2       ],\n",
       "       [671.2       ],\n",
       "       [671.2       ],\n",
       "       [671.2       ],\n",
       "       [671.2       ],\n",
       "       [671.2       ],\n",
       "       [689.1       ],\n",
       "       [689.1       ],\n",
       "       [744.4       ],\n",
       "       [752.6       ],\n",
       "       [752.6       ],\n",
       "       [752.6       ],\n",
       "       [752.6       ],\n",
       "       [752.6       ],\n",
       "       [665.6       ],\n",
       "       [665.6       ],\n",
       "       [665.6       ],\n",
       "       [665.6       ],\n",
       "       [614.7       ],\n",
       "       [703.07754491]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.fit_transform(df[['DC_na']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**【一些实践中的 tips】**  \n",
    "1. 尽量不要把包含个别特征缺失值的样本删除，实践中最好使用一些业务经验来做一些合理的推测值的填充，利用好样本\n",
    "2. 如果没有合适的推测手段来填充，可以填充一些像-999,-1这样的没有意义的值\n",
    "3. 其他一些可能用到的方法:\n",
    "    * np.nan\n",
    "    * np.inf\n",
    "    * df.fillna\n",
    "    * df.replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be used for feature selection/dimensionality reduction on sample sets, either to improve estimators’ accuracy scores or to boost their performance on very high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SelectFromModel is a meta-transformer that can be used along with any estimator that has a coef_ or feature_importances_ attribute after fitting. The features are considered unimportant and removed, if the corresponding coef_ or feature_importances_ values are below the provided threshold parameter. Apart from specifying the threshold numerically, there are built-in heuristics for finding a threshold using a string argument. Available heuristics are “mean”, “median” and float multiples of these like “0.1*mean”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class sklearn.feature_selection.SelectFromModel(estimator, threshold=None, prefit=False, norm_order=1, max_features=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1-based feature selection（实际应用中少于树模型，这里演示用L1正则的模型来选取特征）\n",
    "- Tree-based feature selection（实际应用中优先考虑，这里演示RF）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SelectFromModel in module sklearn.feature_selection._from_model:\n",
      "\n",
      "class SelectFromModel(sklearn.base.MetaEstimatorMixin, sklearn.feature_selection._base.SelectorMixin, sklearn.base.BaseEstimator)\n",
      " |  SelectFromModel(estimator, threshold=None, prefit=False, norm_order=1, max_features=None)\n",
      " |  \n",
      " |  Meta-transformer for selecting features based on importance weights.\n",
      " |  \n",
      " |  .. versionadded:: 0.17\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimator : object\n",
      " |      The base estimator from which the transformer is built.\n",
      " |      This can be both a fitted (if ``prefit`` is set to True)\n",
      " |      or a non-fitted estimator. The estimator must have either a\n",
      " |      ``feature_importances_`` or ``coef_`` attribute after fitting.\n",
      " |  \n",
      " |  threshold : string, float, optional default None\n",
      " |      The threshold value to use for feature selection. Features whose\n",
      " |      importance is greater or equal are kept while the others are\n",
      " |      discarded. If \"median\" (resp. \"mean\"), then the ``threshold`` value is\n",
      " |      the median (resp. the mean) of the feature importances. A scaling\n",
      " |      factor (e.g., \"1.25*mean\") may also be used. If None and if the\n",
      " |      estimator has a parameter penalty set to l1, either explicitly\n",
      " |      or implicitly (e.g, Lasso), the threshold used is 1e-5.\n",
      " |      Otherwise, \"mean\" is used by default.\n",
      " |  \n",
      " |  prefit : bool, default False\n",
      " |      Whether a prefit model is expected to be passed into the constructor\n",
      " |      directly or not. If True, ``transform`` must be called directly\n",
      " |      and SelectFromModel cannot be used with ``cross_val_score``,\n",
      " |      ``GridSearchCV`` and similar utilities that clone the estimator.\n",
      " |      Otherwise train the model using ``fit`` and then ``transform`` to do\n",
      " |      feature selection.\n",
      " |  \n",
      " |  norm_order : non-zero int, inf, -inf, default 1\n",
      " |      Order of the norm used to filter the vectors of coefficients below\n",
      " |      ``threshold`` in the case where the ``coef_`` attribute of the\n",
      " |      estimator is of dimension 2.\n",
      " |  \n",
      " |  max_features : int or None, optional\n",
      " |      The maximum number of features selected scoring above ``threshold``.\n",
      " |      To disable ``threshold`` and only select based on ``max_features``,\n",
      " |      set ``threshold=-np.inf``.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimator_ : an estimator\n",
      " |      The base estimator from which the transformer is built.\n",
      " |      This is stored only when a non-fitted estimator is passed to the\n",
      " |      ``SelectFromModel``, i.e when prefit is False.\n",
      " |  \n",
      " |  threshold_ : float\n",
      " |      The threshold value used for feature selection.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  Allows NaN/Inf in the input if the underlying estimator does as well.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.feature_selection import SelectFromModel\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> X = [[ 0.87, -1.34,  0.31 ],\n",
      " |  ...      [-2.79, -0.02, -0.85 ],\n",
      " |  ...      [-1.34, -0.48, -2.55 ],\n",
      " |  ...      [ 1.92,  1.48,  0.65 ]]\n",
      " |  >>> y = [0, 1, 0, 1]\n",
      " |  >>> selector = SelectFromModel(estimator=LogisticRegression()).fit(X, y)\n",
      " |  >>> selector.estimator_.coef_\n",
      " |  array([[-0.3252302 ,  0.83462377,  0.49750423]])\n",
      " |  >>> selector.threshold_\n",
      " |  0.55245...\n",
      " |  >>> selector.get_support()\n",
      " |  array([False,  True, False])\n",
      " |  >>> selector.transform(X)\n",
      " |  array([[-1.34],\n",
      " |         [-0.02],\n",
      " |         [-0.48],\n",
      " |         [ 1.48]])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SelectFromModel\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.feature_selection._base.SelectorMixin\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator, threshold=None, prefit=False, norm_order=1, max_features=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None, **fit_params)\n",
      " |      Fit the SelectFromModel meta-transformer.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The training input samples.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target values (integers that correspond to classes in\n",
      " |          classification, real numbers in regression).\n",
      " |      \n",
      " |      **fit_params : Other estimator specific parameters\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  partial_fit(self, X, y=None, **fit_params)\n",
      " |      Fit the SelectFromModel meta-transformer only once.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The training input samples.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target values (integers that correspond to classes in\n",
      " |          classification, real numbers in regression).\n",
      " |      \n",
      " |      **fit_params : Other estimator specific parameters\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  threshold_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.feature_selection._base.SelectorMixin:\n",
      " |  \n",
      " |  get_support(self, indices=False)\n",
      " |      Get a mask, or integer index, of the features selected\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indices : boolean (default False)\n",
      " |          If True, the return value will be an array of integers, rather\n",
      " |          than a boolean mask.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      support : array\n",
      " |          An index that selects the retained features from a feature vector.\n",
      " |          If `indices` is False, this is a boolean array of shape\n",
      " |          [# input features], in which an element is True iff its\n",
      " |          corresponding feature is selected for retention. If `indices` is\n",
      " |          True, this is an integer array of shape [# output features] whose\n",
      " |          values are indices into the input feature vector.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Reverse the transformation operation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array of shape [n_samples, n_selected_features]\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_r : array of shape [n_samples, n_original_features]\n",
      " |          `X` with columns of zeros inserted where features would have\n",
      " |          been removed by :meth:`transform`.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Reduce X to the selected features.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array of shape [n_samples, n_features]\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_r : array of shape [n_samples, n_selected_features]\n",
      " |          The input samples with only the selected features.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array of shape [n_samples, n_features]\n",
      " |          Training set.\n",
      " |      \n",
      " |      y : numpy array of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看帮助文档\n",
    "help(SelectFromModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - estimator：对象。构建特征选择实例的基本分类器。如果参数prefit为True，则该参数可以由一个已经训练过的分类器初始化。如果prefit为False，则该参数只能传入没有经过训练的分类器实例  \n",
    "  \n",
    "  \n",
    "> - threshold：字符串，浮点数，（可选的）默认为None。该参数指定特征选择的阈值，词语在分类模型中对应的系数值大于该值时被保留，否则被移除。如果该参数为字符串类型，则可设置的值有”mean”表示系数向量值的均值，”median”表示系数向量值的中值，也可以为”0.x*mean”或”0.x*median”。当该参数设置值为None时，如果分类器具有罚项且罚项设置为l1，则阈值为1e-5，否则该值为”mean”  \n",
    "  \n",
    "  \n",
    "> - prefit：布尔类型。默认值为False。是否对传入的基本分类器事先进行训练。如果设置该值为True，则需要对传入的基本分类器进行训练，如果设置该值为False，则只需要传入分类器实例即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata = modelData.drop(\"area\",axis = 1).fillna(-999)\n",
    "ydata = modelData['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "      normalize=False, positive=False, precompute=False, random_state=None,\n",
       "      selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso()\n",
    "lasso.fit(xdata, ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>...</th>\n",
       "      <th>month_nov</th>\n",
       "      <th>month_oct</th>\n",
       "      <th>month_sep</th>\n",
       "      <th>day_fri</th>\n",
       "      <th>day_mon</th>\n",
       "      <th>day_sat</th>\n",
       "      <th>day_sun</th>\n",
       "      <th>day_thu</th>\n",
       "      <th>day_tue</th>\n",
       "      <th>day_wed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   X  Y  FFMC   DMC     DC  ISI  temp  RH  wind  rain  ...  month_nov  \\\n",
       "0  7  5  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0  ...          0   \n",
       "1  7  4  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0  ...          0   \n",
       "2  7  4  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0  ...          0   \n",
       "3  8  6  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2  ...          0   \n",
       "4  8  6  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0  ...          0   \n",
       "\n",
       "   month_oct  month_sep  day_fri  day_mon  day_sat  day_sun  day_thu  day_tue  \\\n",
       "0          0          0        1        0        0        0        0        0   \n",
       "1          1          0        0        0        0        0        0        1   \n",
       "2          1          0        0        0        1        0        0        0   \n",
       "3          0          0        1        0        0        0        0        0   \n",
       "4          0          0        0        0        0        1        0        0   \n",
       "\n",
       "   day_wed  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.85720489,  0.        , -0.03700954,  0.09217834, -0.01157946,\n",
       "       -0.53603543,  0.72312094, -0.231194  ,  1.26363755, -0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  5.21977984, -0.        , -0.        ,\n",
       "        7.00192208, -0.        ,  0.        ,  0.        , -0.        ])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelectFromModel(lasso, prefit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7. ,  86.2,  26.2, ...,   6.7,   0. ,   0. ],\n",
       "       [  7. ,  90.6,  35.4, ...,   0.9,   0. ,   0. ],\n",
       "       [  7. ,  90.6,  43.7, ...,   1.3,   0. ,   1. ],\n",
       "       ...,\n",
       "       [  7. ,  81.6,  56.7, ...,   6.7,   0. ,   0. ],\n",
       "       [  1. ,  94.4, 146. , ...,   4. ,   0. ,   1. ],\n",
       "       [  6. ,  79.5,   3. , ...,   4.5,   0. ,   0. ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(xdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：实际中，我们对于one-hot处理后的那些列一般不会删除，除非这些列的系数都为0，才会删除**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(xdata, ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.34644708e-02, 4.62341430e-02, 5.39475470e-02, 1.20864442e-01,\n",
       "       3.27448183e-02, 4.69004734e-02, 4.02714863e-01, 6.77922778e-02,\n",
       "       6.71592368e-02, 1.55172745e-04, 3.90417828e-03, 5.71109779e-04,\n",
       "       7.49947160e-03, 2.16358895e-04, 7.54656719e-04, 1.95300809e-08,\n",
       "       8.14230713e-03, 2.60470690e-04, 6.83718444e-04, 9.32061675e-04,\n",
       "       1.04199943e-09, 3.38023628e-04, 8.17767218e-03, 9.13170294e-04,\n",
       "       1.11723857e-02, 2.00352999e-02, 4.47294814e-03, 2.82117074e-02,\n",
       "       7.89782089e-03, 3.83917306e-03])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7. ,  5. , 86.2, ...,  8.2, 51. ,  6.7],\n",
       "       [ 7. ,  4. , 90.6, ..., 18. , 33. ,  0.9],\n",
       "       [ 7. ,  4. , 90.6, ..., 14.6, 33. ,  1.3],\n",
       "       ...,\n",
       "       [ 7. ,  4. , 81.6, ..., 21.2, 70. ,  6.7],\n",
       "       [ 1. ,  4. , 94.4, ..., 25.6, 42. ,  4. ],\n",
       "       [ 6. ,  3. , 79.5, ..., 11.8, 31. ,  4.5]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf = SelectFromModel(rf, prefit=True)\n",
    "model_rf.transform(xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 8)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf.transform(xdata).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 30)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※各特征独立考量\n",
    "  \n",
    "**【注意】很少用，因为实际中很难确定需要设置的阈值/个数/比例**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing features with low variance\n",
    "\n",
    "VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7. ,  5. , 86.2, ...,  8.2, 51. ,  6.7],\n",
       "       [ 7. ,  4. , 90.6, ..., 18. , 33. ,  0.9],\n",
       "       [ 7. ,  4. , 90.6, ..., 14.6, 33. ,  1.3],\n",
       "       ...,\n",
       "       [ 7. ,  4. , 81.6, ..., 21.2, 70. ,  6.7],\n",
       "       [ 1. ,  4. , 94.4, ..., 25.6, 42. ,  4. ],\n",
       "       [ 6. ,  3. , 79.5, ..., 11.8, 31. ,  4.5]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "v = VarianceThreshold(0.5)\n",
    "v.fit_transform(xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 9)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.fit_transform(xdata).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate feature selection\n",
    "1. SelectKBest removes all but the k highest scoring features\n",
    "2. SelectPercentile removes all but a user-specified highest scoring percentage of features\n",
    "\n",
    "These objects take as input a scoring function that returns univariate scores and p-values (or only scores for SelectKBest and SelectPercentile):\n",
    "* For regression: f_regression, mutual_info_regression\n",
    "* For classification: chi2, f_classif, mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SelectKBest in module sklearn.feature_selection._univariate_selection:\n",
      "\n",
      "class SelectKBest(_BaseFilter)\n",
      " |  SelectKBest(score_func=<function f_classif at 0x000000001AAFFBF8>, k=10)\n",
      " |  \n",
      " |  Select features according to the k highest scores.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  score_func : callable\n",
      " |      Function taking two arrays X and y, and returning a pair of arrays\n",
      " |      (scores, pvalues) or a single array with scores.\n",
      " |      Default is f_classif (see below \"See also\"). The default function only\n",
      " |      works with classification tasks.\n",
      " |  \n",
      " |  k : int or \"all\", optional, default=10\n",
      " |      Number of top features to select.\n",
      " |      The \"all\" option bypasses selection, for use in a parameter search.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  scores_ : array-like of shape (n_features,)\n",
      " |      Scores of features.\n",
      " |  \n",
      " |  pvalues_ : array-like of shape (n_features,)\n",
      " |      p-values of feature scores, None if `score_func` returned only scores.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_digits\n",
      " |  >>> from sklearn.feature_selection import SelectKBest, chi2\n",
      " |  >>> X, y = load_digits(return_X_y=True)\n",
      " |  >>> X.shape\n",
      " |  (1797, 64)\n",
      " |  >>> X_new = SelectKBest(chi2, k=20).fit_transform(X, y)\n",
      " |  >>> X_new.shape\n",
      " |  (1797, 20)\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  Ties between features with equal scores will be broken in an unspecified\n",
      " |  way.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  f_classif: ANOVA F-value between label/feature for classification tasks.\n",
      " |  mutual_info_classif: Mutual information for a discrete target.\n",
      " |  chi2: Chi-squared stats of non-negative features for classification tasks.\n",
      " |  f_regression: F-value between label/feature for regression tasks.\n",
      " |  mutual_info_regression: Mutual information for a continuous target.\n",
      " |  SelectPercentile: Select features based on percentile of the highest scores.\n",
      " |  SelectFpr: Select features based on a false positive rate test.\n",
      " |  SelectFdr: Select features based on an estimated false discovery rate.\n",
      " |  SelectFwe: Select features based on family-wise error rate.\n",
      " |  GenericUnivariateSelect: Univariate feature selector with configurable mode.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SelectKBest\n",
      " |      _BaseFilter\n",
      " |      sklearn.feature_selection._base.SelectorMixin\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, score_func=<function f_classif at 0x000000001AAFFBF8>, k=10)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseFilter:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Run score function on (X, y) and get the appropriate features.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The training input samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.feature_selection._base.SelectorMixin:\n",
      " |  \n",
      " |  get_support(self, indices=False)\n",
      " |      Get a mask, or integer index, of the features selected\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indices : boolean (default False)\n",
      " |          If True, the return value will be an array of integers, rather\n",
      " |          than a boolean mask.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      support : array\n",
      " |          An index that selects the retained features from a feature vector.\n",
      " |          If `indices` is False, this is a boolean array of shape\n",
      " |          [# input features], in which an element is True iff its\n",
      " |          corresponding feature is selected for retention. If `indices` is\n",
      " |          True, this is an integer array of shape [# output features] whose\n",
      " |          values are indices into the input feature vector.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Reverse the transformation operation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array of shape [n_samples, n_selected_features]\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_r : array of shape [n_samples, n_original_features]\n",
      " |          `X` with columns of zeros inserted where features would have\n",
      " |          been removed by :meth:`transform`.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Reduce X to the selected features.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array of shape [n_samples, n_features]\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_r : array of shape [n_samples, n_selected_features]\n",
      " |          The input samples with only the selected features.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array of shape [n_samples, n_features]\n",
      " |          Training set.\n",
      " |      \n",
      " |      y : numpy array of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看官方文档\n",
    "help(SelectKBest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7. ,  5. , 86.2, ...,  0. ,  0. ,  0. ],\n",
       "       [ 7. ,  4. , 90.6, ...,  0. ,  0. ,  0. ],\n",
       "       [ 7. ,  4. , 90.6, ...,  1. ,  0. ,  0. ],\n",
       "       ...,\n",
       "       [ 7. ,  4. , 81.6, ...,  0. ,  1. ,  0. ],\n",
       "       [ 1. ,  4. , 94.4, ...,  1. ,  0. ,  0. ],\n",
       "       [ 6. ,  3. , 79.5, ...,  0. ,  0. ,  0. ]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "skb = SelectKBest(f_regression, k=20)\n",
    "skb.fit_transform(xdata, ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 20)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skb.fit_transform(xdata, ydata).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 降维（考虑了所有特征间的整体贡献）  \n",
    "  \n",
    "  **【注意】一般实际应用的其实也较少**\n",
    "  - 难得选取到需要的业务特征\n",
    "  - 机器学习中会使用正则项来惩罚共线性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)  \n",
    "  \n",
    "class sklearn.decomposition.TruncatedSVD(n_components=2, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主成分分析PCA\n",
    "\n",
    "PCA的工作原理是将原始数据集映射到一个新的空间，在这个空间中，矩阵的新列向量是每个正交的。从数据分析的角度来看，PCA将数据的协方差矩阵转化为能够 \"解释 \"一定比例的方差的列向量。\n",
    "\n",
    "- 最大方差解释（保留的特征数越多，方差解释越大）：http://www.cnblogs.com/jerrylead/archive/2011/04/18/2020209.html\n",
    "- 最小平方误差解释：\n",
    "http://www.cnblogs.com/jerrylead/archive/2011/04/18/2020216.html  \n",
    "  \n",
    "【注意】正常数据集其实很少用PCA，用的最多的是在图像压缩上（如只需要抓住图片中主要的人脸部分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9165650429690326"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "# 一定记得要先对数据做标准化，再对其做PCA降维(因为量纲会影响协方差矩阵计算) \n",
    "ss = StandardScaler()\n",
    "xdata_ss = ss.fit_transform(xdata)\n",
    "\n",
    "pca = PCA(20)  # 保留20个主成分\n",
    "pca.fit_transform(xdata_ss)\n",
    "\n",
    "# 查看20个主成分能解释多大的方差比例\n",
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 20)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit_transform(xdata_ss).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated SVD（截断的奇异矩阵分解）  \n",
    "  \n",
    "TruncatedSVD与PCA非常相似，但不同的是，它直接对样本矩阵X进行工作，而不是对其协方差矩阵进行工作。\n",
    "\n",
    "Truncated SVD与普通SVD的不同之处在于，它产生的因子化结果的列数是等于我们指定的截断数的。\n",
    "例如，给定一个n×n矩阵，普通SVD将生成具有n列的矩阵，而截断后的SVD将生成我们指定的列数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.20393887, -0.81969546, -0.46081681, ...,  0.60921858,\n",
       "         0.01381626,  0.16708439],\n",
       "       [-0.02041855,  0.88110739, -0.91015821, ..., -0.20278749,\n",
       "        -1.44123271,  0.20772519],\n",
       "       [ 0.2674066 ,  1.06898908, -1.0189032 , ...,  0.49693935,\n",
       "        -0.40004288,  1.56095058],\n",
       "       ...,\n",
       "       [ 0.50563481,  0.56382819,  2.85012055, ..., -0.12241451,\n",
       "         0.30171322, -0.02277013],\n",
       "       [-1.76737222, -0.79189878,  0.27323316, ...,  0.44888657,\n",
       "         0.2317635 , -0.08663814],\n",
       "       [ 4.23336759,  0.70608683, -0.42100672, ...,  7.92798779,\n",
       "         4.41944341,  6.03773562]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsvd = TruncatedSVD(20)  # 保留20个\n",
    "tsvd.fit_transform(xdata_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 20)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsvd.fit_transform(xdata_ss).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
