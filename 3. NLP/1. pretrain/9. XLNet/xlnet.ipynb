{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLNet: Generalized Autoregressive Pretraining for Language Understanding\n",
    "\n",
    "**参考**：\n",
    "\n",
    "[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)\n",
    "\n",
    "[XLNet (Zihang Dai)](https://github.com/zihangdai/xlnet)\n",
    "\n",
    "[XLNet (Hungging Face)](https://huggingface.co/transformers/model_doc/xlnet.html)\n",
    "\n",
    "[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)\n",
    "\n",
    "**思考**：\n",
    "\n",
    "1. 乱序语言模型的“乱序”指的是什么？输入序列的词序不重要吗？\n",
    "\n",
    "2. 什么是上下文信息？双向语言模型能表示上下文信息吗？乱序语言模型如何学到上下文信息的？\n",
    "\n",
    "## 摘要\n",
    "\n",
    "`XLNet`, a generalized autoregressive pretraining method\n",
    "\n",
    "1. learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order\n",
    "\n",
    "2. overcomes the limitations of BERT\n",
    "\n",
    "![](./slides/slides_07.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 预训练语言模型\n",
    "\n",
    "**预训练语言模型**：通过自监督学习，充分利用大量无监督文本，并将其中的语言知识编码，对下游NLP任务产生积极作用。2018年`ELMo`、`GPT`、`BERT`等工作的发表开启了NLP的新时代；2019年`GPT2`、`Roberta`、`T5`等工作表明，更大规模的数据、更复杂的模型可以不断提高预训练语言模型的性能表现。\n",
    "\n",
    "![](./slides/slides_02.jpg)\n",
    "\n",
    "`word2vec`等词向量模型输出为静态词向量，即不考虑任何上下文信息，每个单词的词向量固定。静态词向量无法描述一词多义现象。\n",
    "\n",
    "相比词向量模型，预训练模型能够根据上下文语境计算单词词向量，即单词词向量是动态向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 自回归目标函数v.s.去噪自编码器目标函数\n",
    "\n",
    "![](./slides/slides_03.jpg)\n",
    "\n",
    "![](./slides/slides_05.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 乱序语言模型（Permutation Language Modeling）\n",
    "\n",
    "if model parameters are shared across all factorization orders, in expectation, the model will learn to gather information from all positions on both sides\n",
    "\n",
    "**乱序（orderless）**：并非指输入序列可以随机排列；而是概率分布无论按何种顺序进行因子分解（factorization orders），模型都能够拟合。\n",
    "\n",
    "![](./slides/slides_09.jpg)\n",
    "\n",
    "![](./slides/slides_11.jpg)\n",
    "\n",
    "![](./slides/slides_14.jpg)\n",
    "\n",
    "![](./slides/slides_15.jpg)\n",
    "\n",
    "![](./img/plm_obj.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 考虑目标位置的表示（Target-Position-Aware Representation）\n",
    "\n",
    "目标位置（target position）指$z_{t}$，乱序后的单词位置索引。\n",
    "\n",
    "![](./slides/slides_22.jpg)\n",
    "\n",
    "![](./slides/slides_24.jpg)\n",
    "\n",
    "![](./slides/slides_28.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 双流自注意力（Two-Stream Self-Attention）机制\n",
    "\n",
    "![](./slides/slides_30.jpg)\n",
    "\n",
    "![](./slides/slides_31.jpg)\n",
    "\n",
    "![](./img/two_stream_self_attention_.png)\n",
    "\n",
    "![](./img/two_stream_self_attention.png)\n",
    "\n",
    "![](./slides/slides_32.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 工程实现\n",
    "\n",
    "### 5.1 部分预测（Partial Prediction）\n",
    "\n",
    "![](./img/parial_pred.png)\n",
    "\n",
    "### 5.2 Transform-XL\n",
    "\n",
    "![](./img/transformer_xl_.png)\n",
    "\n",
    "![](./img/transformer_xl.png)\n",
    "\n",
    "### 5.3 相对段落编码（Relative Segment Encodings）\n",
    "\n",
    "![](./img/rel_seg_enc.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 总结\n",
    "\n",
    "![](./slides/slides_39.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 实验\n",
    "\n",
    "![](./slides/slides_41.jpg)\n",
    "\n",
    "![](./slides/slides_43.jpg)\n",
    "\n",
    "![](./slides/slides_44.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 附录\n",
    "\n",
    "代码实现（摘自[XLNet (Zihang Dai)](https://github.com/zihangdai/xlnet)）\n",
    "\n",
    "### 乱序、掩膜\n",
    "\n",
    "```python\n",
    "def _local_perm(inputs, targets, is_masked, perm_size, seq_len):\n",
    "  \"\"\"\n",
    "  Sample a permutation of the factorization order, and create an\n",
    "  attention mask accordingly.\n",
    "\n",
    "  Args:\n",
    "    inputs: int64 Tensor in shape [seq_len], input ids.\n",
    "    targets: int64 Tensor in shape [seq_len], target ids.\n",
    "    is_masked: bool Tensor in shape [seq_len]. True means being selected\n",
    "      for partial prediction.\n",
    "    perm_size: the length of longest permutation. Could be set to be reuse_len.\n",
    "      Should not be larger than reuse_len or there will be data leaks.\n",
    "    seq_len: int, sequence length.\n",
    "  \"\"\"\n",
    "\n",
    "  # Generate permutation indices\n",
    "  index = tf.range(seq_len, dtype=tf.int64)\n",
    "  index = tf.transpose(tf.reshape(index, [-1, perm_size]))\n",
    "  index = tf.random_shuffle(index)\n",
    "  index = tf.reshape(tf.transpose(index), [-1])\n",
    "\n",
    "  # `perm_mask` and `target_mask`\n",
    "  # non-functional tokens\n",
    "  non_func_tokens = tf.logical_not(tf.logical_or(\n",
    "      tf.equal(inputs, SEP_ID),\n",
    "      tf.equal(inputs, CLS_ID)))\n",
    "\n",
    "  non_mask_tokens = tf.logical_and(tf.logical_not(is_masked), non_func_tokens)\n",
    "  masked_or_func_tokens = tf.logical_not(non_mask_tokens)\n",
    "\n",
    "  # Set the permutation indices of non-masked (& non-funcional) tokens to the\n",
    "  # smallest index (-1):\n",
    "  # (1) they can be seen by all other positions\n",
    "  # (2) they cannot see masked positions, so there won\"t be information leak\n",
    "  smallest_index = -tf.ones([seq_len], dtype=tf.int64)\n",
    "  rev_index = tf.where(non_mask_tokens, smallest_index, index)\n",
    "\n",
    "  # Create `target_mask`: non-funcional and maksed tokens\n",
    "  # 1: use mask as input and have loss\n",
    "  # 0: use token (or [SEP], [CLS]) as input and do not have loss\n",
    "  target_tokens = tf.logical_and(masked_or_func_tokens, non_func_tokens)\n",
    "  target_mask = tf.cast(target_tokens, tf.float32)\n",
    "\n",
    "  # Create `perm_mask`\n",
    "  # `target_tokens` cannot see themselves\n",
    "  self_rev_index = tf.where(target_tokens, rev_index, rev_index + 1)\n",
    "\n",
    "  # 1: cannot attend if i <= j and j is not non-masked (masked_or_func_tokens)\n",
    "  # 0: can attend if i > j or j is non-masked\n",
    "  perm_mask = tf.logical_and(\n",
    "      self_rev_index[:, None] <= rev_index[None, :],\n",
    "      masked_or_func_tokens)\n",
    "  perm_mask = tf.cast(perm_mask, tf.float32)\n",
    "\n",
    "  # new target: [next token] for LM and [curr token] (self) for PLM\n",
    "  new_targets = tf.concat([inputs[0: 1], targets[: -1]],\n",
    "                          axis=0)\n",
    "\n",
    "  # construct inputs_k\n",
    "  inputs_k = inputs\n",
    "\n",
    "  # construct inputs_q\n",
    "  inputs_q = target_mask\n",
    "\n",
    "  return perm_mask, new_targets, target_mask, inputs_k, inputs_q\n",
    "```\n",
    "\n",
    "### 双流自注意力机制\n",
    "\n",
    "```python\n",
    "def two_stream_rel_attn(h, g, r, mems, r_w_bias, r_r_bias, seg_mat, r_s_bias,\n",
    "                        seg_embed, attn_mask_h, attn_mask_g, target_mapping,\n",
    "                        d_model, n_head, d_head, dropout, dropatt, is_training,\n",
    "                        kernel_initializer, scope='rel_attn'):\n",
    "  \"\"\"Two-stream attention with relative positional encoding.\"\"\"\n",
    "\n",
    "  scale = 1 / (d_head ** 0.5)\n",
    "  with tf.variable_scope(scope, reuse=False):\n",
    "\n",
    "    # content based attention score\n",
    "    if mems is not None and mems.shape.ndims > 1:\n",
    "      cat = tf.concat([mems, h], 0)\n",
    "    else:\n",
    "      cat = h\n",
    "\n",
    "    # content-based key head\n",
    "    k_head_h = head_projection(\n",
    "        cat, d_model, n_head, d_head, kernel_initializer, 'k')\n",
    "\n",
    "    # content-based value head\n",
    "    v_head_h = head_projection(\n",
    "        cat, d_model, n_head, d_head, kernel_initializer, 'v')\n",
    "\n",
    "    # position-based key head\n",
    "    k_head_r = head_projection(\n",
    "        r, d_model, n_head, d_head, kernel_initializer, 'r')\n",
    "\n",
    "    ##### h-stream\n",
    "    # content-stream query head\n",
    "    q_head_h = head_projection(\n",
    "        h, d_model, n_head, d_head, kernel_initializer, 'q')\n",
    "\n",
    "    # core attention ops\n",
    "    attn_vec_h = rel_attn_core(\n",
    "        q_head_h, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat, r_w_bias,\n",
    "        r_r_bias, r_s_bias, attn_mask_h, dropatt, is_training, scale)\n",
    "\n",
    "    # post processing\n",
    "    output_h = post_attention(h, attn_vec_h, d_model, n_head, d_head, dropout,\n",
    "                              is_training, kernel_initializer)\n",
    "\n",
    "  with tf.variable_scope(scope, reuse=True):\n",
    "    ##### g-stream\n",
    "    # query-stream query head\n",
    "    q_head_g = head_projection(\n",
    "        g, d_model, n_head, d_head, kernel_initializer, 'q')\n",
    "\n",
    "    # core attention ops\n",
    "    if target_mapping is not None:\n",
    "      q_head_g = tf.einsum('mbnd,mlb->lbnd', q_head_g, target_mapping)\n",
    "      attn_vec_g = rel_attn_core(\n",
    "          q_head_g, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat, r_w_bias,\n",
    "          r_r_bias, r_s_bias, attn_mask_g, dropatt, is_training, scale)\n",
    "      attn_vec_g = tf.einsum('lbnd,mlb->mbnd', attn_vec_g, target_mapping)\n",
    "    else:\n",
    "      attn_vec_g = rel_attn_core(\n",
    "          q_head_g, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat, r_w_bias,\n",
    "          r_r_bias, r_s_bias, attn_mask_g, dropatt, is_training, scale)\n",
    "\n",
    "    # post processing\n",
    "    output_g = post_attention(g, attn_vec_g, d_model, n_head, d_head, dropout,\n",
    "                              is_training, kernel_initializer)\n",
    "\n",
    "    return output_h, output_g\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
