> 参考:
>
> - https://github.com/ZhengZixiang/ABSAPapers
> - https://github.com/songyouwei/ABSA-PyTorch

## attention

​	将Source中的构成元素想象成是由一系列的<Key,Value>数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。

![](https://pic4.zhimg.com/80/v2-a9e91ca7ce873d489f8c68872f1bb38f_720w.jpg)

- Hard attention
- [Soft Attention Model](https://arxiv.org/abs/1409.0473)
- [Global Attention and Local Attention](https://arxiv.org/abs/1508.04025)
- [self attention](https://arxiv.org/abs/1706.03762)
- [Hierarchical Attention](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf)
- [attention over attention](https://arxiv.org/abs/1607.04423)
- [evolving attention](https://arxiv.org/abs/2102.12895)  https://zhuanlan.zhihu.com/p/353605318
- attention遇上推荐系统：https://mp.weixin.qq.com/s/H7PNN9KbtTVN-1bF3qIpgQ
- non-invasive self-attention for side information fusion in sequential recommendation   微信
- stand-alone self-attention in vision models
- transformer-based multi-aspect modeling for multi-aspect multi-sentiment analysis

> 参考：
>
> [重点参考](https://zhuanlan.zhihu.com/p/46727938)

https://github.com/ZhengZixiang/ABSAPapers

https://github.com/songyouwei/ABSA-PyTorch/

https://arrow.tudublin.ie/cgi/viewcontent.cgi?article=1246&context=scschcomdis

https://github.com/yangheng95/LC-ABSA

https://arxiv.org/pdf/1906.03820.pdf

https://github.com/ArrowLuo/GRACE

https://arxiv.org/pdf/2002.09685v3.pdf

https://github.com/shenwzh3/RGAT-ABSA

https://github.com/muyeby/RGAT-ABSA

https://github.com/huminghao16/SpanABSA

## absa-api

https://github.com/ScalaConsultants/Aspect-Based-Sentiment-Analysis



## aspect

| model                                                        | idea                                                         | paper                                                        | data                                                    |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------- |
| review                                                       | 对情感任务进行简介                                           | [Deep Learning for Sentiment Analysis: A Survey](https://arxiv.org/pdf/1801.07883.pdf) |                                                         |
|                                                              | **unified tagging schema**                                   | https://mp.weixin.qq.com/s/Jzra95XfjNtDDTNDMD8Lkw            |                                                         |
|                                                              | unified tagging schema,E2E-ABSA 建模为一个序列标注问题、joint vs pipeline | [A Unified Model for Opinion Target Extraction and Target Sentiment Prediction]()  [code](https://github.com/lixin4ever/E2E-TBSA) | SemEval Laptop\SemEval Restaurant \ Twitter             |
| [more](https://www.cnblogs.com/yifanrensheng/p/13510869.html) | unified tagging schema,E2E-ABSA 建模为一个序列标注问题。     | [Exploiting BERT for End-to-End Aspect-based Sentiment Analysis](https://arxiv.org/pdf/1910.00883.pdf)    [code](https://github.com/lixin4ever/BERT-E2E-ABSA) | SemEval Laptop\SemEval Restaurant \ Twitter             |
|                                                              | 建立在 E2E-ABSA 统一的标注模式（序列标注）下-->无标签    迁移 | [Transferable End-to-End Aspect-based Sentiment Analysis with Selective Adversarial Learning](https://arxiv.org/abs/1910.14192)   [code](https://github.com/hsqmlzno1/Transferable-E2E-ABSA) | Laptop (L) , Restaurant (R) , Device (D) 和 Service (S) |
|                                                              | 三元组问题：What（对于什么），How（情感怎么样）以及 Why（为什么是这个情感） | [Knowing What, How and Why: A Near Complete Solution for Aspect-based Sentiment Analysis (In AAAI 2020)](https://arxiv.org/pdf/1911.01616.pdf)  [code](https://github.com/xuuuluuu/SemEval-Triplet-data) |                                                         |
|                                                              | **ABSA-PyTorch_bert**                                        | https://github.com/songyouwei/ABSA-PyTorch                   |                                                         |

3.29 :Transformer-based Multi-Aspect Modeling for Multi-Aspect Multi-Sentiment Analysis,找到方面情感的思路+数据集

https://github.com/howardhsu/BERT-for-RRC-ABSA

> - [情感分析技术及其应用](https://zhuanlan.zhihu.com/p/354306620)
> - [ABSA简述](https://zhuanlan.zhihu.com/p/354302366)
> - [【情感分析】ABSA模型总结（PART I）](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzIxMzkwNjM2NQ%3D%3D%26mid%3D2247484169%26idx%3D1%26sn%3D31007086d6b42517275415ef3d717ef7%26scene%3D19%23wechat_redirect)
> - [【情感分析】ABSA模型总结（PART II）](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzIxMzkwNjM2NQ%3D%3D%26mid%3D2247484214%26idx%3D1%26sn%3D9d31862263657abb00054c7829c320b8%26scene%3D19%23wechat_redirect)
> - [【情感分析】ABSA模型总结（PART III）](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzIxMzkwNjM2NQ%3D%3D%26mid%3D2247484335%26idx%3D1%26sn%3De4bb3e0eee0faf96ed72fbb5ef9a7d4d%26scene%3D19%26token%3D544241696%26lang%3Dzh_CN%23wechat_redirect)
> - [【情感分析】ABSA模型总结（PART IV）](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzIxMzkwNjM2NQ%3D%3D%26mid%3D2247484533%26idx%3D2%26sn%3D681b4882feb90237dd1762ab064bf254%26chksm%3D97aee2a7a0d96bb1fc5cd4201672204c0fba3378e0ddfb3b1494ef9edb24b97ca8091f76d38a%26token%3D671226418%26lang%3Dzh_CN%23rd)

# ASC

## 一、实验尝试

https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/#linear-layers

### 1. 在LCF-BERT的最后增加conformer

- https://github.com/lucidrains/conformer
- [pytorch维度的变化](https://blog.csdn.net/weicao1990/article/details/93618136?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&dist_request_id=1328741.40132.16170210998309571&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.control)
- 性能没有提高反而下降了：>> test_acc: 0.6500, test_f1: 0.2626

### 2. 无处不在的残差网络 (Residual Nets Family)

- conformer conv+残差连接-->test_acc:0.8607  f1:0.8001
- conformer conv  ->0.843
- 去掉了LCF中的CDW模块，然后替换成了conformer0.8446，0.7783
- 在上面的基础上加上残差 0.8500 0.7836

https://www.bilibili.com/read/cv8243127/

https://blog.csdn.net/qq_42492453/article/details/109464108

### 3. 更改attention

作attention，同时可视化alignment

- 过去都是RNN+attention，加上attention跑不过bert
- LCFS:https://arxiv.org/pdf/2010.07523v2.pdf         
- MGAN
- 
-  **https://github.com/frankaging/Quasi-Attention-ABSA**
- https://github.com/HSLCY/ABSA-BERT-pair
- https://github.com/hsqmlzno1/Transferable-E2E-ABSA
- https://github.com/frankaging/Quasi-Attention-ABSA/blob/main/code/model/CGBERT.py

### 4. joint-network

Multi-grained Attention Network for Aspect-Level Sentiment Classification

https://github.com/fd873630/RNN-Transducer/blob/master/model_rnnt/model.py

### 5. loss

focal loss： https://zhuanlan.zhihu.com/p/308290543  https://arxiv.org/abs/1708.02002

dice loss

更多的loss：https://zhuanlan.zhihu.com/p/258395701

### 6. local self-attention

用local attention + conv：https://arxiv.org/pdf/2103.12731.pdf

https://arxiv.org/pdf/2010.07523v2.pdf

https://github.com/fastnlp/fastNLP/blob/master/fastNLP/modules/encoder/star_transformer.py

> https://github.com/Separius/awesome-fast-attention



## 三、BERT-based models

### 1. BERT-ADA ([official](https://github.com/deepopinion/domain-adapted-atsc))

- bert pretrained model focus on ATSC + cross-domain adaptation between
- 单纯的bert+finetuning  --> 进一步的进行领域自适应

Rietzler, Alexander, et al. "Adapt or get left behind: Domain adaptation through bert language model finetuning for aspect-target sentiment classification." arXiv preprint arXiv:1908.11860 (2019). [[pdf](https://arxiv.org/pdf/1908.11860)]

### 2. BERR-PT ([official](https://github.com/howardhsu/BERT-for-RRC-ABSA))

- proposed a new task called review reading comprehension (RRC)and investigated the possi- bility of turning reviews as a valuable resource for answering user questions. 
- explored the use of this ap- proach in two other review-based tasks: aspect ex- traction and aspect sentiment classification. 

Xu, Hu, et al. "Bert post-training for review reading comprehension and aspect-based sentiment analysis." arXiv preprint arXiv:1904.02232 (2019). [[pdf](https://arxiv.org/pdf/1904.02232)]

### 3. ABSA-BERT-pair ([official](https://github.com/HSLCY/ABSA-BERT-pair))

- via Constructing Auxiliary Sentence

Sun, Chi, Luyao Huang, and Xipeng Qiu. "Utilizing bert for aspect-based sentiment analysis via constructing auxiliary sentence." arXiv preprint arXiv:1903.09588 (2019). [[pdf](https://arxiv.org/pdf/1903.09588.pdf)]

### 4. ￥￥LCF-BERT ([lcf_bert.py](./models/lcf_bert.py)) ([official](https://github.com/yangheng95/LCF-ABSA))  (不错)

- the aspect’s sentiment polarity and the local context. In this paper, a Local Context Focus (LCF)，当前语境
- https://www.aclweb.org/anthology/2020.acl-main.293.pdf
- https://arxiv.org/pdf/1908.01978.pdf

Zeng Biqing, Yang Heng, et al. "LCF: A Local Context Focus Mechanism for Aspect-Based Sentiment Classification." Applied Sciences. 2019, 9, 3389. [[pdf]](https://www.mdpi.com/2076-3417/9/16/3389/pdf)

### 5. ￥￥AEN-BERT ([aen.py](./models/aen.py))(不错)

- without RNN

Song, Youwei, et al. "Attentional Encoder Network for Targeted Sentiment Classification." arXiv preprint arXiv:1902.09314 (2019). [[pdf]](https://arxiv.org/pdf/1902.09314.pdf)

### 6. BERT for Sentence Pair Classification ([bert_spc.py](./models/bert_spc.py))

- vanilla bert

Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." arXiv preprint arXiv:1810.04805 (2018). [[pdf]](https://arxiv.org/pdf/1810.04805.pdf)

### 7. ￥￥LCFS-BERT([github](https://github.com/HieuPhan33/LCFS-BERT))

Modelling Context and Syntactical Features for Aspect-based Sentiment,acl,2020[pdf](https://www.aclweb.org/anthology/2020.acl-main.293.pdf)

### 8. ￥￥ LCF-ATEPC([github](https://github.com/Torilen/TER-LCF-ATEPC-FR))

[A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction](https://arxiv.org/abs/1912.07976)

### 9. Sentio



## 四、 GCN-based models

### 1. ASGCN ([asgcn.py](./models/asgcn.py)) ([official](https://github.com/GeneZC/ASGCN))(不错)

Zhang, Chen, et al. "Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks." Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. 2019. [[pdf]](https://www.aclweb.org/anthology/D19-1464)

### 2. SDGCN-BERT([official](https://github.com/Pinlong-Zhao/SDGCN))

"Modeling Sentiment Dependencies with Graph Convolutional Networks for Aspect-level Sentiment Classification". arXiv preprint arXiv:1906.04501 (2019) [[pdf\]](https://arxiv.org/pdf/1906.04501.pdf)



## 五、RNN-based models+attention

### 1. AOA ([aoa.py](./models/aoa.py))

Huang, Binxuan, et al. "Aspect Level Sentiment Classification with Attention-over-Attention Neural Networks." arXiv preprint arXiv:1804.06536 (2018). [[pdf]](https://arxiv.org/pdf/1804.06536.pdf)

### 2. TNet ([tnet_lf.py](./models/tnet_lf.py)) ([official](https://github.com/lixin4ever/TNet))

Li, Xin, et al. "Transformation Networks for Target-Oriented Sentiment Classification." arXiv preprint arXiv:1805.01086 (2018). [[pdf]](https://arxiv.org/pdf/1805.01086)

### 3. Cabasc ([cabasc.py](./models/cabasc.py))

Liu, Qiao, et al. "Content Attention Model for Aspect Based Sentiment Analysis." Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2018.[pdf](https://zhuanlan.zhihu.com/p/61575551)

### 4. RAM ([ram.py](./models/ram.py))

Chen, Peng, et al. "Recurrent Attention Network on Memory for Aspect Sentiment Analysis." Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 2017. [[pdf]](http://www.aclweb.org/anthology/D17-1047)

### 5. MemNet ([memnet.py](./models/memnet.py)) ([official](https://drive.google.com/open?id=1Hc886aivHmIzwlawapzbpRdTfPoTyi1U))

Tang, Duyu, B. Qin, and T. Liu. "Aspect Level Sentiment Classification with Deep Memory Network." Conference on Empirical Methods in Natural Language Processing 2016:214-224. [[pdf]](https://arxiv.org/pdf/1605.08900)

### 6. ￥￥IAN ([ian.py](./models/ian.py))(不错)

- Interactive Attention Networks交互式网络，可以改进的点
- 上下文+target--> 情感

Ma, Dehong, et al. "Interactive Attention Networks for Aspect-Level Sentiment Classification." arXiv preprint arXiv:1709.00893 (2017). [[pdf]](https://arxiv.org/pdf/1709.00893)

### 7. ATAE-LSTM ([atae_lstm.py](./models/atae_lstm.py))

Wang, Yequan, Minlie Huang, and Li Zhao. "Attention-based lstm for aspect-level sentiment classification." Proceedings of the 2016 conference on empirical methods in natural language processing. 2016.[pdf](https://www.aclweb.org/anthology/D16-1058.pdf)

### 8. TD-LSTM ([td_lstm.py](./models/td_lstm.py), [tc_lstm.py](./models/tc_lstm.py)) ([official](https://drive.google.com/open?id=17RF8MZs456ov9MDiUYZp0SCGL6LvBQl6))

- 论文中提出了两种方式

Tang, Duyu, et al. "Effective LSTMs for Target-Dependent Sentiment Classification." Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. 2016. [[pdf]](https://arxiv.org/pdf/1512.01100)

### 9. LSTM ([lstm.py](./models/lstm.py))

Hochreiter, Sepp, and Jürgen Schmidhuber. "Long short-term memory." Neural computation 9.8 (1997): 1735-1780. [[pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf)]

### 10.MAMS-for-ABSA([github](https://github.com/siat-nlp/MAMS-for-ABSA))

This repository contains the data and code for the paper "A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis", EMNLP-IJCNLP 2019, [[paper\]](https://www.aclweb.org/anthology/D19-1654.pdf).



## 六、GAN-based model

### 1. MGAN ([mgan.py](./models/mgan.py))

Fan, Feifan, et al. "Multi-grained Attention Network for Aspect-Level Sentiment Classification." Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018. [[pdf]](http://aclweb.org/anthology/D18-1380)

https://github.com/howardhsu/BERT-for-RRC-ABSA

roberta https://github.com/HieuPhan33/LCFS-BERT

**idea💡idea**

https://arxiv.org/pdf/2002.09685v3.pdf   https://arxiv.org/pdf/2002.09685v1.pdf

https://www.aclweb.org/anthology/2020.acl-main.295.pdf

Syntax-Aware Graph Attention Network for Aspect-Level Sentiment Classification

## 七、transformer-based model



Transformer-based Multi-Aspect Modeling for Multi-Aspect Multi-Sentiment Analysis,2020,[pdf](https://arxiv.org/pdf/2011.00476.pdf)

