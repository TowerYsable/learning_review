{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于HMM的词性标注 (Part-Of-Speech tagging via HMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.special import logsumexp\n",
    "from scipy.sparse import lil_matrix\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusHelper:\n",
    "    \"\"\"\n",
    "    用于读取并枚举语料库的辅助类\n",
    "    \"\"\"\n",
    "    def __init__(self, path, sent_end_token=\".\"):\n",
    "        \"\"\"\n",
    "        param: path, 语料库路径, 类别string\n",
    "        param: sent_end_token, 句末标点, 类别string\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.token2id = {} # id从0开始\n",
    "        self.id2token = {}\n",
    "        self.tag2id = {}   # id从0开始\n",
    "        self.id2tag = {}\n",
    "        self.sent_end_token = sent_end_token\n",
    "        self.prepare_dict()\n",
    "        \n",
    "        \n",
    "    def read_lines(self):\n",
    "        \"\"\"\n",
    "        读取数据\n",
    "        \n",
    "        return: token和词性, 类别tuple(类别，词性)\n",
    "        \"\"\"\n",
    "        with open(self.path, \"r\") as f:\n",
    "            for line in tqdm(f):\n",
    "                token, pos_tag = line.strip().split(\"/\")\n",
    "                yield token, pos_tag\n",
    "                \n",
    "                \n",
    "    def read_lines2id(self):\n",
    "        \"\"\"\n",
    "        读取数据，并将token和tag转化为id\n",
    "        \"\"\"\n",
    "        for token, pos_tag in self.read_lines():\n",
    "            yield self.token2id[token], self.tag2id[pos_tag]\n",
    "            \n",
    "    def is_end_tokenid(self, token_id):\n",
    "        \"\"\"\n",
    "        判断是否句末标点id\n",
    "        \n",
    "        param: token_id 待验证tokenid，类别int\n",
    "        return: 是否为句末tokenid, 类别bool\n",
    "        \"\"\"\n",
    "        return token_id == self.token2id[self.sent_end_token]\n",
    "    \n",
    "    \n",
    "    def id_to_tags(self, ids):\n",
    "        \"\"\"\n",
    "        将id序列转化为词性标注\n",
    "        \n",
    "        param: ids, 待转化词性id，类别list[int]\n",
    "        return: 词性标注序列, 类别list[string]\n",
    "        \"\"\"\n",
    "        return [self.id2tag[id] for id in ids]\n",
    "    \n",
    "    \n",
    "    def id_to_tokens(self, ids):\n",
    "        \"\"\"\n",
    "        将id序列转化为token序列\n",
    "        \n",
    "        param: ids, 待转化id，类别list[int]\n",
    "        return: token序列, 类别list[string]\n",
    "        \"\"\"\n",
    "        return [self.id2token[id] for id in ids]\n",
    "            \n",
    "                \n",
    "    def _update_dict(self, symbol2id, id2symbol, symbol):\n",
    "        \"\"\"\n",
    "        给定新项，更新词典:\n",
    "        \n",
    "        param: symbol2id, 符号id映射词典, 类型dict\n",
    "        param: id2symbol, id符号映射词典, 类型dict\n",
    "        param: symbol, 待加入符号, 类型string\n",
    "        \"\"\"\n",
    "        new_id = len(symbol2id)\n",
    "        symbol2id[symbol] = new_id\n",
    "        id2symbol[new_id] = symbol\n",
    "        \n",
    "        \n",
    "    def prepare_dict(self):\n",
    "        \"\"\"\n",
    "        根据语料库准备词典\n",
    "        \"\"\"\n",
    "        print(\"Start constructing dictionaries...\")\n",
    "        for token, pos_tag in self.read_lines():\n",
    "            if not token in self.token2id:\n",
    "                self._update_dict(self.token2id, \n",
    "                                  self.id2token,\n",
    "                                  token)\n",
    "                \n",
    "            if not pos_tag in self.tag2id:\n",
    "                self._update_dict(self.tag2id,\n",
    "                                  self.id2tag,\n",
    "                                  pos_tag\n",
    "                                 )        \n",
    "        print(\"Finished construction.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMMPOSTagger:\n",
    "    \"\"\"\n",
    "    HMM 词性标注模型，实现模型的定义，训练和预测等功能\n",
    "    HMM 参数: \n",
    "        初始状态概率向量 pi, \n",
    "        状态转移概率矩阵 A, \n",
    "        观测概率矩阵    B\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, corpus_helper, eps=None):\n",
    "        \"\"\"\n",
    "        param: corpus_helper，语料库辅助类实例，类别CorpusHelper\n",
    "        param: eps, 极小值，用于平滑log计算，类别float\n",
    "        \"\"\"\n",
    "        self.corpus_helper = corpus_helper\n",
    "        self.n_tokens = len(corpus_helper.token2id)\n",
    "        self.n_tags = len(corpus_helper.tag2id)\n",
    "        self.pi = np.zeros(self.n_tags, dtype=np.float)\n",
    "        self.A = np.zeros((self.n_tags, self.n_tags), dtype=np.float)\n",
    "        self.B = lil_matrix((self.n_tags, self.n_tokens), dtype=np.float)\n",
    "        self.eps = np.finfo(float).eps if eps is None else eps\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        训练模型，完成语料库的统计工作       \n",
    "        \"\"\"\n",
    "        \n",
    "        last_tag_id = None # 记录前一个tag，若其值为None则表明当前为新句开始。\n",
    "        for token_id, tag_id in corpus_helper.read_lines2id():\n",
    "            \n",
    "            # 无论如何都要更新B的统计\n",
    "            self.B[tag_id, token_id] += 1\n",
    "            \n",
    "            if last_tag_id is None:\n",
    "                # 若当前是新句子的开始，需要更新pi\n",
    "                self.pi[tag_id] += 1\n",
    "            else:\n",
    "                # 否则，更新A\n",
    "                self.A[last_tag_id, tag_id] += 1\n",
    "                \n",
    "            # 更新上一时刻tag\n",
    "            last_tag_id = None if corpus_helper.is_end_tokenid(token_id) else tag_id\n",
    "            \n",
    "        # 转化为概率\n",
    "        self.pi = self.pi / sum(self.pi)\n",
    "        self.A = self.A / self.A.sum(axis=1).reshape(-1,1)\n",
    "        self.B = self.B / self.B.sum(axis=1).reshape(-1,1)\n",
    "        \n",
    "        print(\"训练结束\")\n",
    "        print(\"pi:{}\".format(self.pi))\n",
    "        print(\"A[0,:]:\\n{}\".format(self.A[0]))\n",
    "        \n",
    "        \n",
    "    def _log(self, p):\n",
    "        \"\"\"\n",
    "        log 函数，考虑平滑\n",
    "        \"\"\"\n",
    "        return np.log(p + self.eps)\n",
    "\n",
    "    \n",
    "    def decode(self, sentence):\n",
    "        \"\"\"\n",
    "        给定句子，使用Viterbi算法找到最佳词性标注序列\n",
    "        \n",
    "        注意！该玩具程序不做未登录词和分词等处理，若需要可自行扩展功能。\n",
    "        \n",
    "        param: sentence, 输入句子, 类型string\n",
    "        return:词性标注序列, 类型list[string]\n",
    "        \"\"\"\n",
    "        if not sentence:\n",
    "            print(\"请输入句子\")\n",
    "            return \"\"\n",
    "        \n",
    "        # (这里没有考虑未登录词的情况)\n",
    "        token_ids = [self.corpus_helper.token2id[token] for token in sentence.split(\" \")]\n",
    "        n_tags, n_tokens = self.n_tags, len(token_ids)\n",
    "        A, B = self.A, self.B\n",
    "        \n",
    "        # 初始化动态规划存储矩阵和记录最佳路径的回溯矩阵\n",
    "        dp = np.zeros((n_tags, n_tokens), dtype=np.float)\n",
    "        traces = np.zeros((n_tags, n_tokens), dtype=np.int)\n",
    "      \n",
    "        # 初始化第一个token的位置\n",
    "        for i in range(n_tags):\n",
    "            dp[i,0] = self._log(self.pi[i]) + self._log(self.B[i,token_ids[0]])\n",
    "            \n",
    "        # 动态规划更新第二个token开始的分数\n",
    "        for t in range(1, n_tokens):\n",
    "            \n",
    "            token_id = token_ids[t] # 当前token id\n",
    "            \n",
    "            for i in range(n_tags):\n",
    "                \n",
    "                dp[i, t] = -sys.maxsize # 初始值为系统最小值\n",
    "                \n",
    "                for k in range(n_tags):\n",
    "                    score = dp[k, t-1] + self._log(A[k, i]) + self._log(B[i, token_id])\n",
    "                    \n",
    "                    if score > dp[i, t]:\n",
    "                        dp[i, t] = score\n",
    "                        traces[i, t] = k\n",
    "\n",
    "        # dp中最佳路径的最终tag\n",
    "        last_best_tag = np.argmax(dp[:, -1])\n",
    "        \n",
    "        # 回溯最佳路径\n",
    "        decoded = [0] * n_tokens \n",
    "        \n",
    "        decoded[-1] = last_best_tag\n",
    "        for t in range(n_tokens-1,0,-1):\n",
    "            last_best_tag = traces[last_best_tag, t]\n",
    "            decoded[t-1] = last_best_tag\n",
    "    \n",
    "        pos_tags = self.corpus_helper.id_to_tags(decoded)\n",
    "        return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "198796it [00:00, 1181194.15it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start constructing dictionaries...\n",
      "Finished construction.\n",
      "Number of tags: 54\n",
      "Number of tokens: 18978\n",
      "{'NNP': 0, ',': 1, 'VBG': 2, 'TO': 3, 'VB': 4, 'NN': 5, 'IN': 6, 'JJ': 7, 'VBD': 8, 'NNS': 9, 'CD': 10, 'CC': 11, 'PRP': 12, 'MD': 13, 'DT': 14, '.': 15, 'VBZ': 16, 'VBN': 17, 'WDT': 18, 'VBP': 19, 'POS': 20, 'RB': 21, '$': 22, 'PRP$': 23, ':': 24, 'JJR': 25, '``': 26, \"''\": 27, 'WP': 28, 'JJS': 29, 'WRB': 30, 'RBR': 31, 'NNPS': 32, 'RP': 33, 'WP$': 34, 'EX': 35, '(': 36, ')': 37, 'PDT': 38, 'RBS': 39, 'FW': 40, 'UH': 41, 'SYM': 42, 'LS': 43, '#': 44, 'VBG|NN': 45, 'JJ|NN': 46, 'RB|IN': 47, 'NNS|NN': 48, 'VBN|JJ': 49, 'VB|NN': 50, 'RBR|JJR': 51, 'NN|NNS': 52, 'JJ|RB': 53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "198796it [00:01, 131180.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练结束\n",
      "pi:[1.81324111e-01 0.00000000e+00 1.00049407e-02 3.33498024e-03\n",
      " 3.95256917e-03 3.68083004e-02 1.11660079e-01 3.66847826e-02\n",
      " 6.17588933e-04 3.81669960e-02 8.76976285e-03 5.18774704e-02\n",
      " 6.02766798e-02 2.47035573e-04 2.17267787e-01 0.00000000e+00\n",
      " 1.48221344e-03 6.05237154e-03 8.64624506e-04 2.47035573e-04\n",
      " 0.00000000e+00 4.73073123e-02 0.00000000e+00 7.16403162e-03\n",
      " 1.72924901e-03 2.09980237e-03 7.53458498e-02 6.36116601e-02\n",
      " 2.59387352e-03 1.85276680e-03 5.92885375e-03 1.97628458e-03\n",
      " 2.84090909e-03 0.00000000e+00 0.00000000e+00 2.71739130e-03\n",
      " 5.92885375e-03 5.92885375e-03 9.88142292e-04 3.70553360e-04\n",
      " 1.23517787e-04 0.00000000e+00 0.00000000e+00 1.85276680e-03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "A[0,:]:\n",
      "[3.79116341e-01 1.41891194e-01 1.29038918e-03 8.25849076e-03\n",
      " 9.80695778e-04 5.28027253e-02 4.27376897e-02 8.72303087e-03\n",
      " 6.72550841e-02 2.33302364e-02 1.99752245e-02 4.11892227e-02\n",
      " 7.22617942e-04 1.20264272e-02 2.73562506e-03 5.57448126e-02\n",
      " 3.79374419e-02 5.16155673e-04 6.19386807e-04 4.23247651e-03\n",
      " 5.35253432e-02 8.36172190e-03 4.64540105e-04 0.00000000e+00\n",
      " 6.55517704e-03 1.03231135e-04 7.74233509e-04 2.63239393e-03\n",
      " 4.12924538e-04 0.00000000e+00 4.64540105e-04 1.03231135e-04\n",
      " 1.58459791e-02 5.16155673e-05 0.00000000e+00 0.00000000e+00\n",
      " 3.92278311e-03 4.07762981e-03 0.00000000e+00 0.00000000e+00\n",
      " 5.16155673e-04 0.00000000e+00 1.03231135e-04 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_helper = CorpusHelper(\"./traindata.txt\")\n",
    "print(\"Number of tags: {}\\nNumber of tokens: {}\".format(len(corpus_helper.tag2id), len(corpus_helper.token2id)))\n",
    "print(corpus_helper.tag2id)\n",
    "\n",
    "tagger = HMMPOSTagger(corpus_helper)\n",
    "tagger.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a good programmer .\n",
      "PRP VBP DT JJ NN .\n"
     ]
    }
   ],
   "source": [
    "sent = \"I am a good programmer .\"\n",
    "pos_tags = tagger.decode(sent)\n",
    "print(sent)\n",
    "print(' '.join(pos_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "198796it [00:00, 1148816.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialHMM(algorithm='viterbi', init_params='ste', n_components=6,\n",
       "               n_iter=10, params='ste',\n",
       "               random_state=RandomState(MT19937) at 0x7F5E908FE8D0,\n",
       "               startprob_prior=1.0, tol=0.01, transmat_prior=1.0,\n",
       "               verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 无监督\n",
    "from hmmlearn import hmm\n",
    "model=hmm.MultinomialHMM(n_components=6)\n",
    "long_sent = [token_id for token_id, _ in corpus_helper.read_lines2id()]\n",
    "model.fit(np.array(long_sent).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.94816506e-01 5.90719636e-10 5.70470715e-05 5.06236832e-03\n",
      " 4.45699806e-14 6.40782416e-05]\n",
      "[[0.16541853 0.1562406  0.17235831 0.18549638 0.16903852 0.15144767]\n",
      " [0.17065134 0.15239057 0.18182866 0.19202188 0.15776145 0.14534611]\n",
      " [0.17404666 0.1509628  0.17516193 0.18857971 0.15463097 0.15661792]\n",
      " [0.18019045 0.17238159 0.16452027 0.14960147 0.16869493 0.16461128]\n",
      " [0.18333828 0.17992215 0.18174878 0.1535702  0.15451066 0.14690993]\n",
      " [0.17038877 0.19275633 0.18047987 0.14129436 0.16840071 0.14667995]]\n",
      "[[1.80216536e-04 9.96836816e-02 3.25359625e-04 ... 7.07893565e-08\n",
      "  1.23378509e-06 6.74999528e-06]\n",
      " [7.48413415e-06 9.37375727e-02 7.39038138e-05 ... 8.06400763e-06\n",
      "  7.71788397e-06 7.05876440e-07]\n",
      " [2.67694359e-05 5.12280412e-02 1.93237799e-04 ... 2.44528069e-06\n",
      "  8.79107102e-06 7.29237967e-06]\n",
      " [5.04934498e-05 1.64695585e-02 3.88238498e-04 ... 4.79079806e-06\n",
      "  5.13306864e-07 8.72847493e-06]\n",
      " [2.89775012e-06 2.16572308e-04 1.91525390e-04 ... 7.48727345e-06\n",
      "  3.53728744e-06 2.27453982e-06]\n",
      " [2.61863784e-05 3.89424028e-02 2.08792132e-04 ... 8.01254438e-06\n",
      "  8.69183452e-06 4.01422498e-06]]\n",
      "\n",
      "=========================\n",
      "I am a good programmer .\n",
      "(-50.12322967002188, array([0, 3, 5, 2, 5, 2]))\n",
      "=========================\n",
      "=========================\n",
      "You have a new book .\n",
      "(-45.17914658795885, array([0, 0, 3, 4, 2, 2]))\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "print(model.startprob_)\n",
    "print(model.transmat_)\n",
    "print(model.emissionprob_)\n",
    "print()\n",
    "sents = [\"I am a good programmer .\", \"You have a new book .\"]\n",
    "for sent in sents:\n",
    "    token_ids = [corpus_helper.token2id[w] for w in sent.split(\" \")] \n",
    "    print(\"=\"*25)\n",
    "    print(sent)\n",
    "    print(model.decode(np.array(token_ids).reshape(-1,1)))\n",
    "    print(\"=\"*25)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
